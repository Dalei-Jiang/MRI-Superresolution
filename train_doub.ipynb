{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2faca300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import pdb\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from utils.data_loading import BasicDataset, CarvanaDataset\n",
    "from utils.dice_score import dice_loss\n",
    "from evaluate import evaluate\n",
    "from unet_linear import UNet_line\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e21fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_low = Path('./data_polar_comb/low/')\n",
    "dir_high = Path('./data_polar_comb/high/')\n",
    "dir_checkpoint = Path('./checkpoints_edge/')\n",
    "final_model_path = Path('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a1dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,\n",
    "              device,\n",
    "              epochs: int = 5,\n",
    "              batch_size: int = 1,\n",
    "              learning_rate: float = 1e-5,\n",
    "              val_percent: float = 0.2,\n",
    "              save_checkpoint: bool = True,\n",
    "              low_scale: float = 1.0,\n",
    "              amp: bool = False):\n",
    "    # 1. Create dataset\n",
    "    try:\n",
    "        dataset = CarvanaDataset(dir_low, dir_high, low_scale)\n",
    "    except (AssertionError, RuntimeError):\n",
    "        dataset = BasicDataset(dir_low, dir_high, low_scale)\n",
    "    # 2. Split into train / validation partitions\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    # 3. Create data loaders\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "    # (Initialize logging)\n",
    "    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n",
    "    experiment.config.update(dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                  val_percent=val_percent, save_checkpoint=save_checkpoint, low_scale=low_scale,\n",
    "                                  amp=amp))\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_checkpoint}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {low_scale}\n",
    "        Mixed Precision: {amp}\n",
    "    ''')\n",
    "\n",
    "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    criterion = nn.L1Loss()\n",
    "    global_step = 0\n",
    "    \n",
    "    g_loss = np.zeros(epochs)\n",
    "    p_loss = np.zeros(epochs)\n",
    "    v_loss = np.zeros(epochs)\n",
    "    # 5. Begin training\n",
    "    div = epochs/10\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                lows = batch['low']\n",
    "                true_highs = batch['high']\n",
    "                idx = batch['idx']\n",
    "                \n",
    "                assert lows.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {lows.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                lows = lows.to(device=device, dtype=torch.float32)\n",
    "                true_highs = true_highs.to(device=device, dtype=torch.long)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=amp):\n",
    "                    highs_pred = net(lows)\n",
    "                    pred = highs_pred.detach().cpu().numpy()[0,:,:,:]\n",
    "                    pred = pred.transpose((1,2,0))\n",
    "                    idx = idx[0].split('/')[2].split('.')[0]\n",
    "                    if epoch % 10 == 0:\n",
    "                        np.save('./data_polar_comb/result/%s.npy'%idx, pred)\n",
    "                    loss = criterion(highs_pred, true_highs)\n",
    "                optimizer.zero_grad()\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "                   \n",
    "                pbar.update(lows.shape[0])\n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "                experiment.log({\n",
    "                    'train loss': loss.item(),\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "        experiment.log({\n",
    "            'epoch loss': epoch_loss\n",
    "        })\n",
    "\n",
    "                \n",
    "        with tqdm(total=n_val, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "            for batch in val_loader:\n",
    "                lows = batch['low']\n",
    "                true_highs = batch['high']\n",
    "                idx = batch['idx']\n",
    "                lows = lows.to(device=device, dtype=torch.float32)\n",
    "                true_highs = true_highs.to(device=device, dtype=torch.long)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=amp):\n",
    "                    highs_pred = net(lows)\n",
    "                    pred = highs_pred.detach().cpu().numpy()[0,:,:,:]\n",
    "                    pred = pred.transpose((1,2,0))\n",
    "                    v_l = criterion(highs_pred, true_highs)\n",
    "                pbar.update(lows.shape[0])\n",
    "                experiment.log({\n",
    "                    'validation loss': loss.item(),\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "                pbar.set_postfix(**{'loss (batch)': v_l.item()})\n",
    "\n",
    "        experiment.log({\n",
    "            'epoch loss': epoch_loss\n",
    "        })\n",
    "            \n",
    "        v_loss[epoch-1] = v_l.item()        \n",
    "        g_loss[epoch-1] = epoch_loss\n",
    "        p_loss[epoch-1] = loss.item()\n",
    "        np.save('./loss/p_loss_polar.npy',p_loss)\n",
    "        np.save('./loss/v_loss_polar.npy',v_loss)\n",
    "        \n",
    "        if epoch == epochs:\n",
    "            torch.save(net.state_dict(), str(final_model_path / 'MODEL_polar.pth'))\n",
    "            logging.info('Final model is saved!')\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(p_loss)\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(v_loss)\n",
    "        elif save_checkpoint:\n",
    "            if epoch % 5 == 0:\n",
    "                torch.save(net.state_dict(), str(final_model_path / 'MODEL_polar.pth'))\n",
    "                logging.info('P model is saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79379cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "looping_epochs = 500\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')\n",
    "    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=looping_epochs, help='Number of epochs')\n",
    "    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')\n",
    "    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=1e-5,\n",
    "                        help='Learning rate', dest='lr')\n",
    "    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')\n",
    "    parser.add_argument('--scale', '-s', type=float, default=1.0, help='Downscaling factor of the images')\n",
    "    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,\n",
    "                        help='Percent of the data that is used as validation (0-100)')\n",
    "    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')\n",
    "    parser.add_argument('--bilinear', action='store_true', default=False, help='Use bilinear upsampling')\n",
    "    parser.add_argument('--classes', '-c', type=int, default=2, help='Number of classes')\n",
    "\n",
    "    return parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5b8a7-20dd-45c5-bfa3-359cd8d32c84",
   "metadata": {},
   "source": [
    "初始化:清除所有已存在的文件数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6ff607-083f-4e70-b227-862a3c65c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "# shutil.rmtree('./Model_edge')\n",
    "# os.mkdir('./Model_edge')\n",
    "# shutil.rmtree('./data_edge/result')\n",
    "# os.mkdir('./data_edge/result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf50d30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cuda\n",
      "INFO: Network:\n",
      "\t2 input channels\n",
      "\t2 output channels (classes)\n",
      "\tTransposed conv upscaling\n",
      "INFO: Creating dataset with 2000 examples\n",
      "ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: anony-mouse-286188. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.19 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daleij2/ondemand/data/sys/dashboard/batch_connect/sys/jupyter-notebook/official/U-net/wandb/run-20220625_081939-r8n8j6r1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/anony-mouse-286188/U-Net/runs/r8n8j6r1?apiKey=8dc47b463872b1e2790603010223e7142d120186\" target=\"_blank\">crisp-paper-349</a></strong> to <a href=\"https://wandb.ai/anony-mouse-286188/U-Net?apiKey=8dc47b463872b1e2790603010223e7142d120186\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting training:\n",
      "        Epochs:          500\n",
      "        Batch size:      1\n",
      "        Learning rate:   1e-05\n",
      "        Training size:   1800\n",
      "        Validation size: 200\n",
      "        Checkpoints:     True\n",
      "        Device:          cuda\n",
      "        Images scaling:  1.0\n",
      "        Mixed Precision: False\n",
      "    \n",
      "Epoch 1/500: 100%|██████████| 1800/1800 [01:02<00:00, 28.89img/s, loss (batch)=29.9]\n",
      "Epoch 1/500: 100%|██████████| 200/200 [00:03<00:00, 62.54img/s, loss (batch)=16.7]\n",
      "Epoch 2/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.24img/s, loss (batch)=25.8]\n",
      "Epoch 2/500: 100%|██████████| 200/200 [00:03<00:00, 64.09img/s, loss (batch)=15.4]\n",
      "Epoch 3/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.16img/s, loss (batch)=23.2]\n",
      "Epoch 3/500: 100%|██████████| 200/200 [00:03<00:00, 62.97img/s, loss (batch)=14.9]\n",
      "Epoch 4/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.22img/s, loss (batch)=22.7]\n",
      "Epoch 4/500: 100%|██████████| 200/200 [00:03<00:00, 64.68img/s, loss (batch)=15.2]\n",
      "Epoch 5/500: 100%|██████████| 1800/1800 [00:58<00:00, 31.02img/s, loss (batch)=20.7]\n",
      "Epoch 5/500: 100%|██████████| 200/200 [00:03<00:00, 63.33img/s, loss (batch)=15.7]\n",
      "INFO: P model is saved!\n",
      "Epoch 6/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.09img/s, loss (batch)=20.4]\n",
      "Epoch 6/500: 100%|██████████| 200/200 [00:03<00:00, 66.40img/s, loss (batch)=15.1]\n",
      "Epoch 7/500: 100%|██████████| 1800/1800 [00:59<00:00, 30.14img/s, loss (batch)=30.8]\n",
      "Epoch 7/500: 100%|██████████| 200/200 [00:03<00:00, 60.42img/s, loss (batch)=15.3]\n",
      "Epoch 8/500: 100%|██████████| 1800/1800 [00:59<00:00, 30.23img/s, loss (batch)=22.9]\n",
      "Epoch 8/500: 100%|██████████| 200/200 [00:03<00:00, 64.60img/s, loss (batch)=15.1]\n",
      "Epoch 9/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.07img/s, loss (batch)=16.8]\n",
      "Epoch 9/500: 100%|██████████| 200/200 [00:03<00:00, 64.08img/s, loss (batch)=15.4]\n",
      "Epoch 10/500: 100%|██████████| 1800/1800 [01:06<00:00, 27.05img/s, loss (batch)=26.4]\n",
      "Epoch 10/500: 100%|██████████| 200/200 [00:03<00:00, 52.10img/s, loss (batch)=14.7]\n",
      "INFO: P model is saved!\n",
      "Epoch 11/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.92img/s, loss (batch)=26.6]\n",
      "Epoch 11/500: 100%|██████████| 200/200 [00:03<00:00, 62.93img/s, loss (batch)=15.5]\n",
      "Epoch 12/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.08img/s, loss (batch)=21.9]\n",
      "Epoch 12/500: 100%|██████████| 200/200 [00:03<00:00, 65.42img/s, loss (batch)=15.1]\n",
      "Epoch 13/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.24img/s, loss (batch)=14.9]\n",
      "Epoch 13/500: 100%|██████████| 200/200 [00:03<00:00, 64.15img/s, loss (batch)=14.7]\n",
      "Epoch 14/500: 100%|██████████| 1800/1800 [00:58<00:00, 31.00img/s, loss (batch)=25.7]\n",
      "Epoch 14/500: 100%|██████████| 200/200 [00:03<00:00, 65.06img/s, loss (batch)=14.9]\n",
      "Epoch 15/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.09img/s, loss (batch)=27.5]\n",
      "Epoch 15/500: 100%|██████████| 200/200 [00:03<00:00, 61.98img/s, loss (batch)=15.3]\n",
      "INFO: P model is saved!\n",
      "Epoch 16/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.80img/s, loss (batch)=31.2]\n",
      "Epoch 16/500: 100%|██████████| 200/200 [00:03<00:00, 63.47img/s, loss (batch)=14.4]\n",
      "Epoch 17/500: 100%|██████████| 1800/1800 [01:00<00:00, 29.75img/s, loss (batch)=16.3]\n",
      "Epoch 17/500: 100%|██████████| 200/200 [00:03<00:00, 63.18img/s, loss (batch)=15.2]\n",
      "Epoch 18/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.06img/s, loss (batch)=16.7]\n",
      "Epoch 18/500: 100%|██████████| 200/200 [00:03<00:00, 64.89img/s, loss (batch)=15.7]\n",
      "Epoch 19/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.24img/s, loss (batch)=30.9]\n",
      "Epoch 19/500: 100%|██████████| 200/200 [00:03<00:00, 63.43img/s, loss (batch)=15.6]\n",
      "Epoch 20/500: 100%|██████████| 1800/1800 [01:12<00:00, 24.92img/s, loss (batch)=29]  \n",
      "Epoch 20/500: 100%|██████████| 200/200 [00:03<00:00, 62.16img/s, loss (batch)=15]  \n",
      "INFO: P model is saved!\n",
      "Epoch 21/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.88img/s, loss (batch)=24.9]\n",
      "Epoch 21/500: 100%|██████████| 200/200 [00:03<00:00, 63.07img/s, loss (batch)=14.5]\n",
      "Epoch 22/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.73img/s, loss (batch)=21.7]\n",
      "Epoch 22/500: 100%|██████████| 200/200 [00:03<00:00, 64.75img/s, loss (batch)=15.1]\n",
      "Epoch 23/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.77img/s, loss (batch)=20.9]\n",
      "Epoch 23/500: 100%|██████████| 200/200 [00:03<00:00, 62.23img/s, loss (batch)=14.8]\n",
      "Epoch 24/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.74img/s, loss (batch)=37.9]\n",
      "Epoch 24/500: 100%|██████████| 200/200 [00:03<00:00, 65.60img/s, loss (batch)=14.7]\n",
      "Epoch 25/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.75img/s, loss (batch)=20.7]\n",
      "Epoch 25/500: 100%|██████████| 200/200 [00:03<00:00, 62.69img/s, loss (batch)=15]  \n",
      "INFO: P model is saved!\n",
      "Epoch 26/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.92img/s, loss (batch)=14.3]\n",
      "Epoch 26/500: 100%|██████████| 200/200 [00:03<00:00, 65.87img/s, loss (batch)=14.5]\n",
      "Epoch 27/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.86img/s, loss (batch)=24.6]\n",
      "Epoch 27/500: 100%|██████████| 200/200 [00:03<00:00, 63.40img/s, loss (batch)=14.8]\n",
      "Epoch 28/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.78img/s, loss (batch)=30.3]\n",
      "Epoch 28/500: 100%|██████████| 200/200 [00:03<00:00, 64.47img/s, loss (batch)=14.4]\n",
      "Epoch 29/500: 100%|██████████| 1800/1800 [01:02<00:00, 28.93img/s, loss (batch)=24.7]\n",
      "Epoch 29/500: 100%|██████████| 200/200 [00:03<00:00, 61.04img/s, loss (batch)=15.1]\n",
      "Epoch 30/500: 100%|██████████| 1800/1800 [01:08<00:00, 26.10img/s, loss (batch)=23.1]\n",
      "Epoch 30/500: 100%|██████████| 200/200 [00:03<00:00, 62.83img/s, loss (batch)=14.9]\n",
      "INFO: P model is saved!\n",
      "Epoch 31/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.88img/s, loss (batch)=37]  \n",
      "Epoch 31/500: 100%|██████████| 200/200 [00:03<00:00, 62.44img/s, loss (batch)=14.9]\n",
      "Epoch 32/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.82img/s, loss (batch)=20]  \n",
      "Epoch 32/500: 100%|██████████| 200/200 [00:03<00:00, 64.94img/s, loss (batch)=15.3]\n",
      "Epoch 33/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.78img/s, loss (batch)=17.5]\n",
      "Epoch 33/500: 100%|██████████| 200/200 [00:03<00:00, 62.97img/s, loss (batch)=14.6]\n",
      "Epoch 34/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.69img/s, loss (batch)=19.5]\n",
      "Epoch 34/500: 100%|██████████| 200/200 [00:03<00:00, 65.09img/s, loss (batch)=15.3]\n",
      "Epoch 35/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.75img/s, loss (batch)=27.1]\n",
      "Epoch 35/500: 100%|██████████| 200/200 [00:03<00:00, 61.59img/s, loss (batch)=15.8]\n",
      "INFO: P model is saved!\n",
      "Epoch 36/500: 100%|██████████| 1800/1800 [01:00<00:00, 29.62img/s, loss (batch)=30.6]\n",
      "Epoch 36/500: 100%|██████████| 200/200 [00:03<00:00, 64.43img/s, loss (batch)=15.1]\n",
      "Epoch 37/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.69img/s, loss (batch)=26.1]\n",
      "Epoch 37/500: 100%|██████████| 200/200 [00:03<00:00, 62.04img/s, loss (batch)=15.3]\n",
      "Epoch 38/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.77img/s, loss (batch)=25.6]\n",
      "Epoch 38/500: 100%|██████████| 200/200 [00:03<00:00, 65.01img/s, loss (batch)=14.9]\n",
      "Epoch 39/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.77img/s, loss (batch)=12.4]\n",
      "Epoch 39/500: 100%|██████████| 200/200 [00:03<00:00, 63.18img/s, loss (batch)=15]  \n",
      "Epoch 40/500: 100%|██████████| 1800/1800 [01:09<00:00, 26.07img/s, loss (batch)=12.8]\n",
      "Epoch 40/500: 100%|██████████| 200/200 [00:03<00:00, 63.57img/s, loss (batch)=14.9]\n",
      "INFO: P model is saved!\n",
      "Epoch 41/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.75img/s, loss (batch)=24.6]\n",
      "Epoch 41/500: 100%|██████████| 200/200 [00:03<00:00, 61.80img/s, loss (batch)=15.2]\n",
      "Epoch 42/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.69img/s, loss (batch)=19.9]\n",
      "Epoch 42/500: 100%|██████████| 200/200 [00:03<00:00, 64.38img/s, loss (batch)=15.2]\n",
      "Epoch 43/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.61img/s, loss (batch)=27.4]\n",
      "Epoch 43/500: 100%|██████████| 200/200 [00:03<00:00, 63.37img/s, loss (batch)=15.5]\n",
      "Epoch 44/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.82img/s, loss (batch)=34.6]\n",
      "Epoch 44/500: 100%|██████████| 200/200 [00:03<00:00, 65.14img/s, loss (batch)=14.6]\n",
      "Epoch 45/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.84img/s, loss (batch)=28.2]\n",
      "Epoch 45/500: 100%|██████████| 200/200 [00:03<00:00, 62.18img/s, loss (batch)=15.3]\n",
      "INFO: P model is saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.79img/s, loss (batch)=20.6]\n",
      "Epoch 46/500: 100%|██████████| 200/200 [00:03<00:00, 65.26img/s, loss (batch)=14.6]\n",
      "Epoch 47/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.99img/s, loss (batch)=14.3]\n",
      "Epoch 47/500: 100%|██████████| 200/200 [00:03<00:00, 63.81img/s, loss (batch)=16]  \n",
      "Epoch 48/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.64img/s, loss (batch)=16.5]\n",
      "Epoch 48/500: 100%|██████████| 200/200 [00:03<00:00, 62.72img/s, loss (batch)=14.7]\n",
      "Epoch 49/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.82img/s, loss (batch)=24.3]\n",
      "Epoch 49/500: 100%|██████████| 200/200 [00:03<00:00, 63.25img/s, loss (batch)=15.1]\n",
      "Epoch 50/500: 100%|██████████| 1800/1800 [01:08<00:00, 26.32img/s, loss (batch)=18.7]\n",
      "Epoch 50/500: 100%|██████████| 200/200 [00:03<00:00, 65.53img/s, loss (batch)=15.4]\n",
      "INFO: P model is saved!\n",
      "Epoch 51/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.83img/s, loss (batch)=13.5]\n",
      "Epoch 51/500: 100%|██████████| 200/200 [00:03<00:00, 62.79img/s, loss (batch)=15]  \n",
      "Epoch 52/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.88img/s, loss (batch)=20]  \n",
      "Epoch 52/500: 100%|██████████| 200/200 [00:03<00:00, 65.60img/s, loss (batch)=14.8]\n",
      "Epoch 53/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.97img/s, loss (batch)=31.9]\n",
      "Epoch 53/500: 100%|██████████| 200/200 [00:03<00:00, 63.22img/s, loss (batch)=15.5]\n",
      "Epoch 54/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.88img/s, loss (batch)=19.6]\n",
      "Epoch 54/500: 100%|██████████| 200/200 [00:03<00:00, 64.05img/s, loss (batch)=14.8]\n",
      "Epoch 55/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.99img/s, loss (batch)=29.7]\n",
      "Epoch 55/500: 100%|██████████| 200/200 [00:03<00:00, 62.92img/s, loss (batch)=14.5]\n",
      "INFO: P model is saved!\n",
      "Epoch 56/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.96img/s, loss (batch)=26.2]\n",
      "Epoch 56/500: 100%|██████████| 200/200 [00:03<00:00, 62.84img/s, loss (batch)=15.1]\n",
      "Epoch 57/500: 100%|██████████| 1800/1800 [00:58<00:00, 31.00img/s, loss (batch)=29.1]\n",
      "Epoch 57/500: 100%|██████████| 200/200 [00:03<00:00, 62.91img/s, loss (batch)=14.8]\n",
      "Epoch 58/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.76img/s, loss (batch)=16.2]\n",
      "Epoch 58/500: 100%|██████████| 200/200 [00:03<00:00, 62.15img/s, loss (batch)=14]  \n",
      "Epoch 59/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.95img/s, loss (batch)=16]  \n",
      "Epoch 59/500: 100%|██████████| 200/200 [00:03<00:00, 62.56img/s, loss (batch)=15]  \n",
      "Epoch 60/500: 100%|██████████| 1800/1800 [01:13<00:00, 24.41img/s, loss (batch)=22.9]\n",
      "Epoch 60/500: 100%|██████████| 200/200 [00:03<00:00, 65.12img/s, loss (batch)=14.6]\n",
      "INFO: P model is saved!\n",
      "Epoch 61/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.90img/s, loss (batch)=25]  \n",
      "Epoch 61/500: 100%|██████████| 200/200 [00:03<00:00, 63.03img/s, loss (batch)=14.7]\n",
      "Epoch 62/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.82img/s, loss (batch)=27]  \n",
      "Epoch 62/500: 100%|██████████| 200/200 [00:03<00:00, 65.00img/s, loss (batch)=15.2]\n",
      "Epoch 63/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.75img/s, loss (batch)=24.1]\n",
      "Epoch 63/500: 100%|██████████| 200/200 [00:03<00:00, 63.49img/s, loss (batch)=14.4]\n",
      "Epoch 64/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.54img/s, loss (batch)=21.6]\n",
      "Epoch 64/500: 100%|██████████| 200/200 [00:03<00:00, 64.48img/s, loss (batch)=14.4]\n",
      "Epoch 65/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.79img/s, loss (batch)=27.1]\n",
      "Epoch 65/500: 100%|██████████| 200/200 [00:03<00:00, 62.81img/s, loss (batch)=14.5]\n",
      "INFO: P model is saved!\n",
      "Epoch 66/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.76img/s, loss (batch)=19.1]\n",
      "Epoch 66/500: 100%|██████████| 200/200 [00:03<00:00, 63.86img/s, loss (batch)=14.8]\n",
      "Epoch 67/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.91img/s, loss (batch)=32.1]\n",
      "Epoch 67/500: 100%|██████████| 200/200 [00:03<00:00, 62.94img/s, loss (batch)=14.9]\n",
      "Epoch 68/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.82img/s, loss (batch)=37.9]\n",
      "Epoch 68/500: 100%|██████████| 200/200 [00:03<00:00, 62.62img/s, loss (batch)=15]  \n",
      "Epoch 69/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.80img/s, loss (batch)=16.8]\n",
      "Epoch 69/500: 100%|██████████| 200/200 [00:03<00:00, 63.03img/s, loss (batch)=14.7]\n",
      "Epoch 70/500: 100%|██████████| 1800/1800 [01:11<00:00, 25.22img/s, loss (batch)=25]  \n",
      "Epoch 70/500: 100%|██████████| 200/200 [00:03<00:00, 65.03img/s, loss (batch)=14.8]\n",
      "INFO: P model is saved!\n",
      "Epoch 71/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.83img/s, loss (batch)=22.9]\n",
      "Epoch 71/500: 100%|██████████| 200/200 [00:03<00:00, 63.11img/s, loss (batch)=15.2]\n",
      "Epoch 72/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.81img/s, loss (batch)=38.7]\n",
      "Epoch 72/500: 100%|██████████| 200/200 [00:03<00:00, 65.45img/s, loss (batch)=14.2]\n",
      "Epoch 73/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.82img/s, loss (batch)=20.6]\n",
      "Epoch 73/500: 100%|██████████| 200/200 [00:03<00:00, 62.93img/s, loss (batch)=14.7]\n",
      "Epoch 74/500: 100%|██████████| 1800/1800 [00:59<00:00, 30.30img/s, loss (batch)=25.1]\n",
      "Epoch 74/500: 100%|██████████| 200/200 [00:03<00:00, 64.43img/s, loss (batch)=14.4]\n",
      "Epoch 75/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.11img/s, loss (batch)=24.4]\n",
      "Epoch 75/500: 100%|██████████| 200/200 [00:03<00:00, 63.81img/s, loss (batch)=14.7]\n",
      "INFO: P model is saved!\n",
      "Epoch 76/500: 100%|██████████| 1800/1800 [00:58<00:00, 31.02img/s, loss (batch)=23.1]\n",
      "Epoch 76/500: 100%|██████████| 200/200 [00:03<00:00, 65.23img/s, loss (batch)=14.3]\n",
      "Epoch 77/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.13img/s, loss (batch)=23.6]\n",
      "Epoch 77/500: 100%|██████████| 200/200 [00:03<00:00, 63.07img/s, loss (batch)=14.7]\n",
      "Epoch 78/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.80img/s, loss (batch)=14.9]\n",
      "Epoch 78/500: 100%|██████████| 200/200 [00:03<00:00, 62.71img/s, loss (batch)=14.7]\n",
      "Epoch 79/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.23img/s, loss (batch)=19.1]\n",
      "Epoch 79/500: 100%|██████████| 200/200 [00:03<00:00, 63.94img/s, loss (batch)=14.6]\n",
      "Epoch 80/500: 100%|██████████| 1800/1800 [01:12<00:00, 24.94img/s, loss (batch)=17.9]\n",
      "Epoch 80/500: 100%|██████████| 200/200 [00:03<00:00, 62.86img/s, loss (batch)=14.9]\n",
      "INFO: P model is saved!\n",
      "Epoch 81/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.99img/s, loss (batch)=23.3]\n",
      "Epoch 81/500: 100%|██████████| 200/200 [00:03<00:00, 63.30img/s, loss (batch)=14.4]\n",
      "Epoch 82/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.21img/s, loss (batch)=21.5]\n",
      "Epoch 82/500: 100%|██████████| 200/200 [00:03<00:00, 64.78img/s, loss (batch)=14.9]\n",
      "Epoch 83/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.96img/s, loss (batch)=14.6]\n",
      "Epoch 83/500: 100%|██████████| 200/200 [00:03<00:00, 63.60img/s, loss (batch)=15.1]\n",
      "Epoch 84/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.97img/s, loss (batch)=19.9]\n",
      "Epoch 84/500: 100%|██████████| 200/200 [00:03<00:00, 65.40img/s, loss (batch)=14.1]\n",
      "Epoch 85/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.91img/s, loss (batch)=30.8]\n",
      "Epoch 85/500: 100%|██████████| 200/200 [00:03<00:00, 64.34img/s, loss (batch)=14.5]\n",
      "INFO: P model is saved!\n",
      "Epoch 86/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.83img/s, loss (batch)=24.2]\n",
      "Epoch 86/500: 100%|██████████| 200/200 [00:03<00:00, 65.58img/s, loss (batch)=14.8]\n",
      "Epoch 87/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.88img/s, loss (batch)=22.5]\n",
      "Epoch 87/500: 100%|██████████| 200/200 [00:03<00:00, 64.00img/s, loss (batch)=14.5]\n",
      "Epoch 88/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.95img/s, loss (batch)=26.4]\n",
      "Epoch 88/500: 100%|██████████| 200/200 [00:03<00:00, 62.79img/s, loss (batch)=14.6]\n",
      "Epoch 89/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.06img/s, loss (batch)=22.9]\n",
      "Epoch 89/500: 100%|██████████| 200/200 [00:03<00:00, 65.40img/s, loss (batch)=14.3]\n",
      "Epoch 90/500: 100%|██████████| 1800/1800 [01:14<00:00, 24.08img/s, loss (batch)=24.6]\n",
      "Epoch 90/500: 100%|██████████| 200/200 [00:03<00:00, 65.80img/s, loss (batch)=14.5]\n",
      "INFO: P model is saved!\n",
      "Epoch 91/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.18img/s, loss (batch)=19.6]\n",
      "Epoch 91/500: 100%|██████████| 200/200 [00:03<00:00, 64.13img/s, loss (batch)=15]  \n",
      "Epoch 92/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.93img/s, loss (batch)=16.7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/500: 100%|██████████| 200/200 [00:03<00:00, 66.02img/s, loss (batch)=15.3]\n",
      "Epoch 93/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.22img/s, loss (batch)=20]  \n",
      "Epoch 93/500: 100%|██████████| 200/200 [00:03<00:00, 63.98img/s, loss (batch)=15.4]\n",
      "Epoch 94/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.88img/s, loss (batch)=30]  \n",
      "Epoch 94/500: 100%|██████████| 200/200 [00:03<00:00, 62.25img/s, loss (batch)=14.7]\n",
      "Epoch 95/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.78img/s, loss (batch)=13.4]\n",
      "Epoch 95/500: 100%|██████████| 200/200 [00:03<00:00, 62.85img/s, loss (batch)=14.2]\n",
      "INFO: P model is saved!\n",
      "Epoch 96/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.76img/s, loss (batch)=24.2]\n",
      "Epoch 96/500: 100%|██████████| 200/200 [00:03<00:00, 62.86img/s, loss (batch)=14.7]\n",
      "Epoch 97/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.94img/s, loss (batch)=23.1]\n",
      "Epoch 97/500: 100%|██████████| 200/200 [00:03<00:00, 61.84img/s, loss (batch)=14.4]\n",
      "Epoch 98/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.96img/s, loss (batch)=16]  \n",
      "Epoch 98/500: 100%|██████████| 200/200 [00:03<00:00, 63.37img/s, loss (batch)=15.1]\n",
      "Epoch 99/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.91img/s, loss (batch)=21.9]\n",
      "Epoch 99/500: 100%|██████████| 200/200 [00:03<00:00, 63.46img/s, loss (batch)=14.1]\n",
      "Epoch 100/500: 100%|██████████| 1800/1800 [01:10<00:00, 25.51img/s, loss (batch)=22.7]\n",
      "Epoch 100/500: 100%|██████████| 200/200 [00:03<00:00, 62.91img/s, loss (batch)=15]  \n",
      "INFO: P model is saved!\n",
      "Epoch 101/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.17img/s, loss (batch)=29.3]\n",
      "Epoch 101/500: 100%|██████████| 200/200 [00:03<00:00, 62.79img/s, loss (batch)=14.8]\n",
      "Epoch 102/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.89img/s, loss (batch)=21]  \n",
      "Epoch 102/500: 100%|██████████| 200/200 [00:03<00:00, 63.03img/s, loss (batch)=14.6]\n",
      "Epoch 103/500: 100%|██████████| 1800/1800 [00:59<00:00, 30.40img/s, loss (batch)=23.1]\n",
      "Epoch 103/500: 100%|██████████| 200/200 [00:03<00:00, 62.38img/s, loss (batch)=14.9]\n",
      "Epoch 104/500: 100%|██████████| 1800/1800 [00:58<00:00, 30.83img/s, loss (batch)=18.8]\n",
      "Epoch 104/500: 100%|██████████| 200/200 [00:03<00:00, 63.02img/s, loss (batch)=14.8]\n",
      "Epoch 105/500: 100%|██████████| 1800/1800 [00:57<00:00, 31.08img/s, loss (batch)=35.9]\n",
      "Epoch 105/500: 100%|██████████| 200/200 [00:03<00:00, 62.21img/s, loss (batch)=14.7]\n",
      "INFO: P model is saved!\n",
      "Epoch 106/500: 100%|██████████| 1800/1800 [01:02<00:00, 29.01img/s, loss (batch)=37.1]\n",
      "Epoch 106/500: 100%|██████████| 200/200 [00:03<00:00, 56.10img/s, loss (batch)=14.7]\n",
      "Epoch 107/500: 100%|██████████| 1800/1800 [01:31<00:00, 19.60img/s, loss (batch)=14.3]\n",
      "Epoch 107/500: 100%|██████████| 200/200 [00:05<00:00, 39.58img/s, loss (batch)=14.8]\n",
      "Epoch 108/500: 100%|██████████| 1800/1800 [02:12<00:00, 13.61img/s, loss (batch)=20.7]\n",
      "Epoch 108/500: 100%|██████████| 200/200 [00:06<00:00, 29.65img/s, loss (batch)=14.9]\n",
      "Epoch 109/500: 100%|██████████| 1800/1800 [01:33<00:00, 19.16img/s, loss (batch)=21.9]\n",
      "Epoch 109/500: 100%|██████████| 200/200 [00:06<00:00, 33.22img/s, loss (batch)=14.7]\n",
      "Epoch 110/500: 100%|██████████| 1800/1800 [01:49<00:00, 16.44img/s, loss (batch)=23.9]\n",
      "Epoch 110/500: 100%|██████████| 200/200 [00:03<00:00, 62.38img/s, loss (batch)=14.7]\n",
      "INFO: P model is saved!\n",
      "Epoch 111/500: 100%|██████████| 1800/1800 [01:23<00:00, 21.61img/s, loss (batch)=12]  \n",
      "Epoch 111/500: 100%|██████████| 200/200 [00:07<00:00, 26.81img/s, loss (batch)=14.6]\n",
      "Epoch 112/500: 100%|██████████| 1800/1800 [02:22<00:00, 12.65img/s, loss (batch)=18.1]\n",
      "Epoch 112/500: 100%|██████████| 200/200 [00:07<00:00, 27.32img/s, loss (batch)=14.7]\n",
      "Epoch 113/500: 100%|██████████| 1800/1800 [01:46<00:00, 16.97img/s, loss (batch)=15.9]\n",
      "Epoch 113/500: 100%|██████████| 200/200 [00:06<00:00, 32.93img/s, loss (batch)=14.8]\n",
      "Epoch 114/500: 100%|██████████| 1800/1800 [01:30<00:00, 19.87img/s, loss (batch)=15.5]\n",
      "Epoch 114/500: 100%|██████████| 200/200 [00:03<00:00, 55.21img/s, loss (batch)=15]  \n",
      "Epoch 115/500: 100%|██████████| 1800/1800 [01:22<00:00, 21.91img/s, loss (batch)=23.2]\n",
      "Epoch 115/500: 100%|██████████| 200/200 [00:04<00:00, 44.86img/s, loss (batch)=14.9]\n",
      "INFO: P model is saved!\n",
      "Epoch 116/500: 100%|██████████| 1800/1800 [02:04<00:00, 14.49img/s, loss (batch)=15.7]\n",
      "Epoch 116/500: 100%|██████████| 200/200 [00:10<00:00, 19.71img/s, loss (batch)=14.7]\n",
      "Epoch 117/500:  36%|███▋      | 653/1800 [00:48<01:25, 13.42img/s, loss (batch)=26]  \n",
      "INFO: Saved interrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3839588/2384778801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     train_net(net=net,\n\u001b[0m\u001b[1;32m     18\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3839588/341474728.py\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, device, epochs, batch_size, learning_rate, val_percent, save_checkpoint, low_scale, amp)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mgrad_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mgrad_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mgrad_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.0.0/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \"\"\"\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"closure\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.0.0/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/opence-v1.0.0/lib/python3.8/site-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                     \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f'Using device {device}')\n",
    "net = UNet_line(n_channels=2, n_classes=args.classes, bilinear=args.bilinear) #args.classes, bilinear=args.bilinear)\n",
    "logging.info(f'Network:\\n'\n",
    "              f'\\t{net.n_channels} input channels\\n'\n",
    "              f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "              f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n",
    "# args.load = './Completed Model/MODEL_cart.pth'\n",
    "args.load = ''\n",
    "if args.load:\n",
    "    net.load_state_dict(torch.load(args.load, map_location=device))\n",
    "    logging.info(f'Model loaded from {args.load}')\n",
    "net.to(device=device)\n",
    "try:\n",
    "    train_net(net=net,\n",
    "              epochs=args.epochs,\n",
    "              batch_size=args.batch_size,\n",
    "              learning_rate=args.lr,\n",
    "              device=device,\n",
    "              low_scale=args.scale,\n",
    "              val_percent=args.val / 100,\n",
    "              amp=args.amp)\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "    logging.info('Saved interrupt')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ac59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.0.0]",
   "language": "python",
   "name": "conda-env-opence-v1.0.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
