{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2faca300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import pdb\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_loading import BasicDataset, CarvanaDataset\n",
    "from utils.dice_score import dice_loss\n",
    "from evaluate import evaluate\n",
    "from unet import UNet\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f07651",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_low = Path('./data/data_image_origin/low/')\n",
    "dir_high = Path('./data/data_image_origin/high/')\n",
    "dir_checkpoint = Path('./checkpoint/')\n",
    "final_model_path = Path('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05a1dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net,\n",
    "              device,\n",
    "              epochs: int = 5,\n",
    "              batch_size: int = 1,\n",
    "              learning_rate: float = 1e-5,\n",
    "              val_percent: float = 0.2,\n",
    "              save_checkpoint: bool = True,\n",
    "              low_scale: float = 1.0,\n",
    "              amp: bool = False):\n",
    "    # 1. Create dataset\n",
    "    try:\n",
    "        dataset = CarvanaDataset(dir_low, dir_high, low_scale)\n",
    "    except (AssertionError, RuntimeError):\n",
    "        dataset = BasicDataset(dir_low, dir_high, low_scale)\n",
    "    # 2. Split into train / validation partitions\n",
    "    n_val = int(len(dataset) * val_percent)\n",
    "    n_train = len(dataset) - n_val\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    # 3. Create data loaders\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "    train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "    # (Initialize logging)\n",
    "    experiment = wandb.init(project='U-Net', resume='allow', anonymous='must')\n",
    "    experiment.config.update(dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
    "                                  val_percent=val_percent, save_checkpoint=save_checkpoint, low_scale=low_scale,\n",
    "                                  amp=amp))\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_checkpoint}\n",
    "        Device:          {device.type}\n",
    "        Images scaling:  {low_scale}\n",
    "        Mixed Precision: {amp}\n",
    "    ''')\n",
    "\n",
    "    # 4. Set up the optimizer, the loss, the learning rate scheduler and the loss scaling for AMP\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=1e-8, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
    "    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    global_step = 0\n",
    "    \n",
    "    g_loss = np.zeros(epochs)\n",
    "    p_loss = np.zeros(epochs)\n",
    "    v_loss = np.zeros(epochs)\n",
    "    # 5. Begin training\n",
    "    div = epochs/10\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                lows = batch['low']\n",
    "                true_highs = batch['high']\n",
    "                idx = batch['idx']\n",
    "                \n",
    "                assert lows.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                lows = lows.to(device=device, dtype=torch.float32)\n",
    "                true_highs = true_highs.to(device=device, dtype=torch.long)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=amp):\n",
    "                    highs_pred = net(lows)\n",
    "                    pred = highs_pred.detach().cpu().numpy()[0,0,:,:]\n",
    "                    idx = idx[0].split('/')[3].split('.')[0]\n",
    "                    if epoch % 10 == 0:\n",
    "                        np.save('./data/data_image_origin/result/%s.npy'%idx, pred)\n",
    "                    loss = criterion(highs_pred, true_highs)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                grad_scaler.scale(loss).backward()\n",
    "                grad_scaler.step(optimizer)\n",
    "                grad_scaler.update()\n",
    "\n",
    "                pbar.update(lows.shape[0])\n",
    "                global_step += 1\n",
    "                epoch_loss += loss.item()\n",
    "                experiment.log({\n",
    "                    'train loss': loss.item(),\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "                \n",
    "# validation part          \n",
    "        with tqdm(total=n_val, desc=f'Test {epoch}/{epochs}', unit='img') as pbar:\n",
    "            for batch in val_loader:\n",
    "                lows = batch['low']\n",
    "                true_highs = batch['high']\n",
    "                idx = batch['idx']\n",
    "            \n",
    "                assert lows.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {images.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                lows = lows.to(device=device, dtype=torch.float32)\n",
    "                true_highs = true_highs.to(device=device, dtype=torch.long)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=amp):\n",
    "                    highs_pred = net(lows)\n",
    "                    pred = highs_pred.detach().cpu().numpy()[0,0,:,:]\n",
    "                    val_loss = criterion(highs_pred, true_highs)\n",
    "\n",
    "                pbar.update(lows.shape[0])\n",
    "                \n",
    "                experiment.log({\n",
    "                    'validation loss': loss.item(),\n",
    "                    'step': global_step,\n",
    "                    'epoch': epoch\n",
    "                })\n",
    "                pbar.set_postfix(**{'loss (batch)': val_loss.item()})\n",
    "\n",
    "        experiment.log({\n",
    "            'epoch loss': epoch_loss\n",
    "        })\n",
    "        g_loss[epoch-1] = epoch_loss\n",
    "        p_loss[epoch-1] = loss.item()        \n",
    "        v_loss[epoch-1] = val_loss.item()\n",
    "        np.save('./loss/p_ori.npy',p_loss)\n",
    "        np.save('./loss/v_ori.npy',v_loss)\n",
    "        if epoch == epochs:\n",
    "            torch.save(net.state_dict(), str(final_model_path / 'MODEL_ori_abs.pth'))\n",
    "            logging.info('Final model is saved!')\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(p_loss)     \n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot(v_loss)\n",
    "        elif save_checkpoint:\n",
    "            if epoch % 10 == 0:\n",
    "                torch.save(net.state_dict(), str(final_model_path / 'MODEL_ori_abs.pth'))\n",
    "                logging.info('P model is saved!')\n",
    "            if epoch % 50 == 0:\n",
    "                Path(dir_checkpoint).mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(net.state_dict(), str(dir_checkpoint / 'checkpoint_epoch{}.pth'.format(epoch//50+9)))\n",
    "                logging.info(f'Checkpoint {epoch} saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79379cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "looping_epochs =500\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Train the UNet on images and target masks')\n",
    "    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=looping_epochs, help='Number of epochs')\n",
    "    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='Batch size')\n",
    "    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=1e-5,\n",
    "                        help='Learning rate', dest='lr')\n",
    "    parser.add_argument('--load', '-f', type=str, default=False, help='Load model from a .pth file')\n",
    "    parser.add_argument('--scale', '-s', type=float, default=1.0, help='Downscaling factor of the images')\n",
    "    parser.add_argument('--validation', '-v', dest='val', type=float, default=1.0,\n",
    "                        help='Percent of the data that is used as validation (0-100)')\n",
    "    parser.add_argument('--amp', action='store_true', default=False, help='Use mixed precision')\n",
    "    parser.add_argument('--bilinear', action='store_true', default=False, help='Use bilinear upsampling')\n",
    "    parser.add_argument('--classes', '-c', type=int, default=1, help='Number of classes')\n",
    "\n",
    "    return parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5b8a7-20dd-45c5-bfa3-359cd8d32c84",
   "metadata": {},
   "source": [
    "初始化:清除所有已存在的文件数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6ff607-083f-4e70-b227-862a3c65c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "# shutil.rmtree('./Model')\n",
    "# os.mkdir('./Model')\n",
    "# shutil.rmtree('./checkpoints')\n",
    "# os.mkdir('./checkpoints')\n",
    "# shutil.rmtree('./data/result')\n",
    "# os.mkdir('./data/result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf50d30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using device cuda\n",
      "INFO: Network:\n",
      "\t1 input channels\n",
      "\t1 output channels (classes)\n",
      "\tTransposed conv upscaling\n",
      "INFO: Model loaded from MODEL_ori_abs.pth\n",
      "INFO: Creating dataset with 4000 examples\n",
      "ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: anony-mouse-286188. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daleij2/ondemand/data/sys/dashboard/batch_connect/sys/jupyter-notebook/official/U-net/wandb/run-20220716_121832-2ixbhwoz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/anony-mouse-286188/U-Net/runs/2ixbhwoz?apiKey=8dc47b463872b1e2790603010223e7142d120186\" target=\"_blank\">jolly-dream-440</a></strong> to <a href=\"https://wandb.ai/anony-mouse-286188/U-Net?apiKey=8dc47b463872b1e2790603010223e7142d120186\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Starting training:\n",
      "        Epochs:          500\n",
      "        Batch size:      1\n",
      "        Learning rate:   1e-05\n",
      "        Training size:   3960\n",
      "        Validation size: 40\n",
      "        Checkpoints:     True\n",
      "        Device:          cuda\n",
      "        Images scaling:  1.0\n",
      "        Mixed Precision: False\n",
      "    \n",
      "Epoch 1/500: 100%|██████████| 3960/3960 [02:16<00:00, 29.00img/s, loss (batch)=3.79]\n",
      "Test 1/500: 100%|██████████| 40/40 [00:00<00:00, 48.04img/s, loss (batch)=2.48]\n",
      "Epoch 2/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.22img/s, loss (batch)=3.36]\n",
      "Test 2/500: 100%|██████████| 40/40 [00:00<00:00, 49.82img/s, loss (batch)=2.44]\n",
      "Epoch 3/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.19img/s, loss (batch)=5.01]\n",
      "Test 3/500: 100%|██████████| 40/40 [00:00<00:00, 51.68img/s, loss (batch)=2.44]\n",
      "Epoch 4/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.22img/s, loss (batch)=1.79]\n",
      "Test 4/500: 100%|██████████| 40/40 [00:00<00:00, 50.35img/s, loss (batch)=2.52]\n",
      "Epoch 5/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.25img/s, loss (batch)=1.7] \n",
      "Test 5/500: 100%|██████████| 40/40 [00:00<00:00, 50.32img/s, loss (batch)=2.41]\n",
      "Epoch 6/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.21img/s, loss (batch)=1.43]\n",
      "Test 6/500: 100%|██████████| 40/40 [00:00<00:00, 49.93img/s, loss (batch)=2.41]\n",
      "Epoch 7/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=1.56]\n",
      "Test 7/500: 100%|██████████| 40/40 [00:00<00:00, 49.27img/s, loss (batch)=2.48]\n",
      "Epoch 8/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.21img/s, loss (batch)=3.52]\n",
      "Test 8/500: 100%|██████████| 40/40 [00:00<00:00, 48.72img/s, loss (batch)=2.53]\n",
      "Epoch 9/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.19img/s, loss (batch)=2.52]\n",
      "Test 9/500: 100%|██████████| 40/40 [00:00<00:00, 49.83img/s, loss (batch)=2.37]\n",
      "Epoch 10/500: 100%|██████████| 3960/3960 [03:51<00:00, 17.09img/s, loss (batch)=1.71]\n",
      "Test 10/500: 100%|██████████| 40/40 [00:00<00:00, 50.42img/s, loss (batch)=2.61]\n",
      "INFO: P model is saved!\n",
      "Epoch 11/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.23img/s, loss (batch)=4.57]\n",
      "Test 11/500: 100%|██████████| 40/40 [00:00<00:00, 50.72img/s, loss (batch)=2.35]\n",
      "Epoch 12/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=2.32]\n",
      "Test 12/500: 100%|██████████| 40/40 [00:00<00:00, 48.29img/s, loss (batch)=2.6] \n",
      "Epoch 13/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=4.33]\n",
      "Test 13/500: 100%|██████████| 40/40 [00:00<00:00, 46.81img/s, loss (batch)=2.56]\n",
      "Epoch 14/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=4.5] \n",
      "Test 14/500: 100%|██████████| 40/40 [00:00<00:00, 49.73img/s, loss (batch)=2.51]\n",
      "Epoch 15/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.25img/s, loss (batch)=1.57]\n",
      "Test 15/500: 100%|██████████| 40/40 [00:00<00:00, 50.18img/s, loss (batch)=2.52]\n",
      "Epoch 16/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.22img/s, loss (batch)=1.67]\n",
      "Test 16/500: 100%|██████████| 40/40 [00:00<00:00, 49.49img/s, loss (batch)=2.55]\n",
      "Epoch 17/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=4.92]\n",
      "Test 17/500: 100%|██████████| 40/40 [00:00<00:00, 49.48img/s, loss (batch)=2.66]\n",
      "Epoch 18/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.20img/s, loss (batch)=1.98]\n",
      "Test 18/500: 100%|██████████| 40/40 [00:00<00:00, 51.28img/s, loss (batch)=2.47]\n",
      "Epoch 19/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=1.54]\n",
      "Test 19/500: 100%|██████████| 40/40 [00:00<00:00, 50.75img/s, loss (batch)=2.5] \n",
      "Epoch 20/500: 100%|██████████| 3960/3960 [02:41<00:00, 24.47img/s, loss (batch)=2.33]\n",
      "Test 20/500: 100%|██████████| 40/40 [00:00<00:00, 49.03img/s, loss (batch)=2.43]\n",
      "INFO: P model is saved!\n",
      "Epoch 21/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.22img/s, loss (batch)=4.48]\n",
      "Test 21/500: 100%|██████████| 40/40 [00:00<00:00, 47.99img/s, loss (batch)=2.5] \n",
      "Epoch 22/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=1.92]\n",
      "Test 22/500: 100%|██████████| 40/40 [00:00<00:00, 49.10img/s, loss (batch)=2.53]\n",
      "Epoch 23/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.13img/s, loss (batch)=3.45]\n",
      "Test 23/500: 100%|██████████| 40/40 [00:00<00:00, 50.33img/s, loss (batch)=2.62]\n",
      "Epoch 24/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.21img/s, loss (batch)=2.79]\n",
      "Test 24/500: 100%|██████████| 40/40 [00:00<00:00, 46.72img/s, loss (batch)=2.56]\n",
      "Epoch 25/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.19img/s, loss (batch)=2.48]\n",
      "Test 25/500: 100%|██████████| 40/40 [00:00<00:00, 49.52img/s, loss (batch)=2.58]\n",
      "Epoch 26/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.15img/s, loss (batch)=1.63]\n",
      "Test 26/500: 100%|██████████| 40/40 [00:00<00:00, 50.64img/s, loss (batch)=2.46]\n",
      "Epoch 27/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=3.64]\n",
      "Test 27/500: 100%|██████████| 40/40 [00:00<00:00, 49.47img/s, loss (batch)=2.53]\n",
      "Epoch 28/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.20img/s, loss (batch)=4.47]\n",
      "Test 28/500: 100%|██████████| 40/40 [00:00<00:00, 49.74img/s, loss (batch)=2.47]\n",
      "Epoch 29/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.20img/s, loss (batch)=4.07]\n",
      "Test 29/500: 100%|██████████| 40/40 [00:00<00:00, 47.50img/s, loss (batch)=2.45]\n",
      "Epoch 30/500: 100%|██████████| 3960/3960 [02:42<00:00, 24.39img/s, loss (batch)=2.53]\n",
      "Test 30/500: 100%|██████████| 40/40 [00:00<00:00, 50.75img/s, loss (batch)=2.52]\n",
      "INFO: P model is saved!\n",
      "Epoch 31/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=3.01]\n",
      "Test 31/500: 100%|██████████| 40/40 [00:00<00:00, 50.47img/s, loss (batch)=2.62]\n",
      "Epoch 32/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=3.61]\n",
      "Test 32/500: 100%|██████████| 40/40 [00:00<00:00, 49.14img/s, loss (batch)=2.61]\n",
      "Epoch 33/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=5.16]\n",
      "Test 33/500: 100%|██████████| 40/40 [00:00<00:00, 50.80img/s, loss (batch)=2.55]\n",
      "Epoch 34/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=1.52]\n",
      "Test 34/500: 100%|██████████| 40/40 [00:00<00:00, 49.52img/s, loss (batch)=2.61]\n",
      "Epoch 35/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=1.61]\n",
      "Test 35/500: 100%|██████████| 40/40 [00:00<00:00, 49.91img/s, loss (batch)=2.53]\n",
      "Epoch 36/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.15img/s, loss (batch)=5.18]\n",
      "Test 36/500: 100%|██████████| 40/40 [00:00<00:00, 49.09img/s, loss (batch)=2.48]\n",
      "Epoch 37/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=2.44]\n",
      "Test 37/500: 100%|██████████| 40/40 [00:00<00:00, 50.72img/s, loss (batch)=2.71]\n",
      "Epoch 38/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=3.36]\n",
      "Test 38/500: 100%|██████████| 40/40 [00:00<00:00, 50.81img/s, loss (batch)=2.67]\n",
      "Epoch 39/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=1.67]\n",
      "Test 39/500: 100%|██████████| 40/40 [00:00<00:00, 48.11img/s, loss (batch)=2.69]\n",
      "Epoch 40/500: 100%|██████████| 3960/3960 [02:41<00:00, 24.55img/s, loss (batch)=2.67]\n",
      "Test 40/500: 100%|██████████| 40/40 [00:00<00:00, 49.54img/s, loss (batch)=2.62]\n",
      "INFO: P model is saved!\n",
      "Epoch 41/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.20img/s, loss (batch)=1.63]\n",
      "Test 41/500: 100%|██████████| 40/40 [00:00<00:00, 48.70img/s, loss (batch)=2.63]\n",
      "Epoch 42/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=3.49]\n",
      "Test 42/500: 100%|██████████| 40/40 [00:00<00:00, 49.97img/s, loss (batch)=2.52]\n",
      "Epoch 43/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=2.26]\n",
      "Test 43/500: 100%|██████████| 40/40 [00:00<00:00, 50.80img/s, loss (batch)=2.52]\n",
      "Epoch 44/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=2.49]\n",
      "Test 44/500: 100%|██████████| 40/40 [00:00<00:00, 49.44img/s, loss (batch)=2.45]\n",
      "Epoch 45/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=2.05]\n",
      "Test 45/500: 100%|██████████| 40/40 [00:00<00:00, 48.98img/s, loss (batch)=2.5] \n",
      "Epoch 46/500: 100%|██████████| 3960/3960 [02:16<00:00, 29.09img/s, loss (batch)=5]   \n",
      "Test 46/500: 100%|██████████| 40/40 [00:00<00:00, 49.76img/s, loss (batch)=2.48]\n",
      "Epoch 47/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=1.39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test 47/500: 100%|██████████| 40/40 [00:00<00:00, 50.18img/s, loss (batch)=2.41]\n",
      "Epoch 48/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=5.02]\n",
      "Test 48/500: 100%|██████████| 40/40 [00:00<00:00, 48.26img/s, loss (batch)=2.75]\n",
      "Epoch 49/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=2.38]\n",
      "Test 49/500: 100%|██████████| 40/40 [00:00<00:00, 49.96img/s, loss (batch)=2.6] \n",
      "Epoch 50/500: 100%|██████████| 3960/3960 [02:37<00:00, 25.13img/s, loss (batch)=4.22]\n",
      "Test 50/500: 100%|██████████| 40/40 [00:00<00:00, 50.27img/s, loss (batch)=2.76]\n",
      "INFO: P model is saved!\n",
      "INFO: Checkpoint 50 saved!\n",
      "Epoch 51/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.21img/s, loss (batch)=2.71]\n",
      "Test 51/500: 100%|██████████| 40/40 [00:00<00:00, 50.40img/s, loss (batch)=2.6] \n",
      "Epoch 52/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.21img/s, loss (batch)=1.93]\n",
      "Test 52/500: 100%|██████████| 40/40 [00:00<00:00, 49.13img/s, loss (batch)=2.47]\n",
      "Epoch 53/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=1.52]\n",
      "Test 53/500: 100%|██████████| 40/40 [00:00<00:00, 47.66img/s, loss (batch)=2.61]\n",
      "Epoch 54/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=1.55]\n",
      "Test 54/500: 100%|██████████| 40/40 [00:00<00:00, 50.09img/s, loss (batch)=2.83]\n",
      "Epoch 55/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=1.52]\n",
      "Test 55/500: 100%|██████████| 40/40 [00:00<00:00, 49.87img/s, loss (batch)=2.48]\n",
      "Epoch 56/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=2.07]\n",
      "Test 56/500: 100%|██████████| 40/40 [00:00<00:00, 48.92img/s, loss (batch)=2.63]\n",
      "Epoch 57/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=3.59]\n",
      "Test 57/500: 100%|██████████| 40/40 [00:00<00:00, 50.19img/s, loss (batch)=2.64]\n",
      "Epoch 58/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=1.97]\n",
      "Test 58/500: 100%|██████████| 40/40 [00:00<00:00, 50.24img/s, loss (batch)=2.52]\n",
      "Epoch 59/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.15img/s, loss (batch)=1.6] \n",
      "Test 59/500: 100%|██████████| 40/40 [00:00<00:00, 48.29img/s, loss (batch)=2.46]\n",
      "Epoch 60/500: 100%|██████████| 3960/3960 [02:36<00:00, 25.32img/s, loss (batch)=2.08]\n",
      "Test 60/500: 100%|██████████| 40/40 [00:00<00:00, 50.61img/s, loss (batch)=2.55]\n",
      "INFO: P model is saved!\n",
      "Epoch 61/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.19img/s, loss (batch)=2.64]\n",
      "Test 61/500: 100%|██████████| 40/40 [00:00<00:00, 48.82img/s, loss (batch)=2.68]\n",
      "Epoch 62/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=4.28]\n",
      "Test 62/500: 100%|██████████| 40/40 [00:00<00:00, 48.68img/s, loss (batch)=2.5] \n",
      "Epoch 63/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.19img/s, loss (batch)=2.13]\n",
      "Test 63/500: 100%|██████████| 40/40 [00:00<00:00, 49.61img/s, loss (batch)=2.54]\n",
      "Epoch 64/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=1.48]\n",
      "Test 64/500: 100%|██████████| 40/40 [00:00<00:00, 50.18img/s, loss (batch)=2.5] \n",
      "Epoch 65/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=2.23]\n",
      "Test 65/500: 100%|██████████| 40/40 [00:00<00:00, 49.38img/s, loss (batch)=2.7] \n",
      "Epoch 66/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.17img/s, loss (batch)=1.78]\n",
      "Test 66/500: 100%|██████████| 40/40 [00:00<00:00, 48.58img/s, loss (batch)=2.62]\n",
      "Epoch 67/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.15img/s, loss (batch)=2.92]\n",
      "Test 67/500: 100%|██████████| 40/40 [00:00<00:00, 51.03img/s, loss (batch)=2.82]\n",
      "Epoch 68/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=2.2] \n",
      "Test 68/500: 100%|██████████| 40/40 [00:00<00:00, 51.62img/s, loss (batch)=2.68]\n",
      "Epoch 69/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.13img/s, loss (batch)=5.48]\n",
      "Test 69/500: 100%|██████████| 40/40 [00:00<00:00, 47.90img/s, loss (batch)=2.64]\n",
      "Epoch 70/500: 100%|██████████| 3960/3960 [02:37<00:00, 25.17img/s, loss (batch)=6.61]\n",
      "Test 70/500: 100%|██████████| 40/40 [00:00<00:00, 48.76img/s, loss (batch)=2.49]\n",
      "INFO: P model is saved!\n",
      "Epoch 71/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=1.87]\n",
      "Test 71/500: 100%|██████████| 40/40 [00:00<00:00, 49.96img/s, loss (batch)=2.73]\n",
      "Epoch 72/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=2.82]\n",
      "Test 72/500: 100%|██████████| 40/40 [00:00<00:00, 49.31img/s, loss (batch)=2.76]\n",
      "Epoch 73/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.18img/s, loss (batch)=1.8] \n",
      "Test 73/500: 100%|██████████| 40/40 [00:00<00:00, 48.31img/s, loss (batch)=2.62]\n",
      "Epoch 74/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.12img/s, loss (batch)=2.69]\n",
      "Test 74/500: 100%|██████████| 40/40 [00:00<00:00, 49.05img/s, loss (batch)=2.57]\n",
      "Epoch 75/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.16img/s, loss (batch)=1.93]\n",
      "Test 75/500: 100%|██████████| 40/40 [00:00<00:00, 49.33img/s, loss (batch)=2.51]\n",
      "Epoch 76/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.15img/s, loss (batch)=2.98]\n",
      "Test 76/500: 100%|██████████| 40/40 [00:00<00:00, 50.86img/s, loss (batch)=2.55]\n",
      "Epoch 77/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=1.36]\n",
      "Test 77/500: 100%|██████████| 40/40 [00:00<00:00, 49.63img/s, loss (batch)=2.52]\n",
      "Epoch 78/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.14img/s, loss (batch)=1.71]\n",
      "Test 78/500: 100%|██████████| 40/40 [00:00<00:00, 49.40img/s, loss (batch)=2.57]\n",
      "Epoch 79/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.19img/s, loss (batch)=1.41]\n",
      "Test 79/500: 100%|██████████| 40/40 [00:00<00:00, 48.94img/s, loss (batch)=2.66]\n",
      "Epoch 80/500: 100%|██████████| 3960/3960 [02:49<00:00, 23.40img/s, loss (batch)=3.5] \n",
      "Test 80/500: 100%|██████████| 40/40 [00:00<00:00, 48.81img/s, loss (batch)=2.6] \n",
      "INFO: P model is saved!\n",
      "Epoch 81/500: 100%|██████████| 3960/3960 [02:15<00:00, 29.12img/s, loss (batch)=1.76]\n",
      "Test 81/500: 100%|██████████| 40/40 [00:00<00:00, 49.22img/s, loss (batch)=2.57]\n",
      "Epoch 82/500:  61%|██████    | 2419/3960 [01:22<00:50, 30.26img/s, loss (batch)=1.32]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "    net = UNet(n_channels=1, n_classes=args.classes, bilinear=args.bilinear) #args.classes, bilinear=args.bilinear)\n",
    "\n",
    "    logging.info(f'Network:\\n'\n",
    "                  f'\\t{net.n_channels} input channels\\n'\n",
    "                  f'\\t{net.n_classes} output channels (classes)\\n'\n",
    "                  f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n",
    "\n",
    "#     args.load ='./Completed Model/MODEL_ori_abs_714.pth'\n",
    "    args.load = 'MODEL_ori_abs.pth'\n",
    "#     args.load = ''\n",
    "    if args.load:\n",
    "        net.load_state_dict(torch.load(args.load, map_location=device))\n",
    "        logging.info(f'Model loaded from {args.load}')\n",
    "    net.to(device=device)\n",
    "    try:\n",
    "        train_net(net=net,\n",
    "                  epochs=args.epochs,\n",
    "                  batch_size=args.batch_size,\n",
    "                  learning_rate=args.lr,\n",
    "                  device=device,\n",
    "                  low_scale=args.scale,\n",
    "                  val_percent=args.val / 100,\n",
    "                  amp=args.amp)\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
    "        logging.info('Saved interrupt')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e838e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.0.0]",
   "language": "python",
   "name": "conda-env-opence-v1.0.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
